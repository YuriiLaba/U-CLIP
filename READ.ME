TODO list:

- train model with 100k data and unfreezing only projection layers
- train projection layers from scratch
  - try separate only text projection
  - try both projections
- try other text backbone (mordern liberta [https://huggingface.co/Goader/modern-liberta-large])
  - train just new projection
  - train projection and model
- create a smart sample (sample more wiki and multi30k data less laion)

- compare modern clip-finetune papers and create comparison table


THE FINSAL GOAL:
- run all experiments that desribed before
- get a results on five datasets (our Wiki, Multi30k eval, Laion, V-WSD, some image classification dataset)
- upplaod the best model to the HuggingFace as best ukranian CLIP
